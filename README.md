# ğŸ§  WhisperVision â€” Real-Time Lip Reading from Silent Video

WhisperVision is a deep learning pipeline that reads lips and predicts spoken sentences using only silent video input. Built using 3D CNNs, BiLSTMs, and CTC decoding, this system enables voice-free communication, real-time inference, and silent speech recognition â€” no audio needed.

---

## ğŸ“¦ Dataset

- **Source**: GRID corpus (publicly available lip-reading dataset)
- **Format**: Video clips (`.mpg`) with sentence-level annotations (`.align`)
- **Structure**:
  - Each video: 75 frames, grayscale, cropped to mouth region
  - Each label: Cleaned, character-level transcription of spoken sentence

---

## ğŸ§± Model Architecture

| Layer              | Details                             |
|-------------------|-------------------------------------|
| Input              | 75 grayscale frames per sequence    |
| 3D Conv Blocks     | Spatial-temporal feature extraction |
| Bi-directional LSTM| Sequence modeling                   |
| Dense + Softmax    | Character probabilities             |
| CTC Loss           | Alignment-free transcription        |

- **Loss**: Connectionist Temporal Classification (CTC)
- **Decoder**: Greedy CTC decoding (`ctc_decode`)

---

## ğŸ§ª Evaluation

| Metric | Score  |
|--------|--------|
| CER    | 22%    |
| WER    | 41%    |

- Tested on ~300 sentence-level samples  
- Evaluated using edit distance between ground truth and predicted transcription

---

## ğŸ”§ Tech Stack

- Python, TensorFlow  
- OpenCV, NumPy  
- Streamlit (for web app)  
- ImageIO, EditDistance

---

## ğŸ›  Project Structure

```
ğŸ“ dataset/
    â”œâ”€â”€ videos/         # Silent input videos
    â””â”€â”€ alignments/     # Sentence-level transcriptions

ğŸ“ model/
    â””â”€â”€ saved_model/    # Trained weights

ğŸ“ utils/
    â”œâ”€â”€ utils.py        # Preprocessing functions
    â”œâ”€â”€ model_utils.py  # Model builder and decoder
    â””â”€â”€ app.py          # Streamlit frontend (upload + prediction)

ğŸ“„ whispervision.ipynb  # Training & Evaluation notebook
```

---

## ğŸš€ Inference Pipeline

1. ğŸ“¥ Upload silent video (`.mpg`)
2. ğŸ¥ Preprocess & extract 75 frames
3. ğŸ§  Feed into trained model
4. ğŸ§¾ Decode output characters (CTC)
5. âœ… Display predicted sentence in real time

---

## ğŸŒ Streamlit App

Run the app locally:

```bash
streamlit run app.py
```

Upload any silent `.mpg` video from GRID dataset and see the real-time transcription.

---

## ğŸ’¡ Use Cases

- Voice-free communication in noisy or secure environments  
- Speech-to-text accessibility for hearing-impaired users  
- Silent surveillance & lip-reading systems  
- Multimodal NLP/vision research

---

## ğŸ§ª Error Metrics Explained

**CER (Character Error Rate)**  
Measures the percentage of characters incorrectly predicted, including insertions, deletions, and substitutions.

```
CER = (Substitutions + Insertions + Deletions) / Total Characters
```

Used for fine-grained accuracy on character-level predictions.

**WER (Word Error Rate)**  
Measures the percentage of words incorrectly predicted â€” similar to CER but at the word level.

```
WER = (Substitutions + Insertions + Deletions) / Total Words
```

Useful for evaluating overall sentence readability and semantic accuracy.

---

### ğŸ” Example:
Ground Truth: â€œplease set the channelâ€  
Predicted  : â€œplease set the channilâ€

- CER = 1 error / 21 chars = ~4.8%  
- WER = 1 error / 4 words = 25%
